{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68adf7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016, Yarin Gal, All rights reserved.\n",
    "# This code is based on the code by Jose Miguel Hernandez-Lobato used for his \n",
    "# paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\".\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import math\n",
    "from scipy.special import logsumexp\n",
    "import numpy as np\n",
    "\n",
    "from keras.regularizers import l2\n",
    "from keras import Input\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Softmax\n",
    "from keras import Model\n",
    "import tensorflow as tf\n",
    "\n",
    "import time\n",
    "\n",
    "\n",
    "class net:\n",
    "\n",
    "    def __init__(self, X_train, y_train, n_hidden, n_epochs = 40,\n",
    "        normalize = False, tau = 1.0, dropout = 0.05):\n",
    "\n",
    "        \"\"\"\n",
    "            Constructor for the class implementing a Bayesian neural network\n",
    "            trained with the probabilistic back propagation method.\n",
    "\n",
    "            @param X_train      Matrix with the features for the training data.\n",
    "            @param y_train      Vector with the target variables for the\n",
    "                                training data.\n",
    "            @param n_hidden     Vector with the number of neurons for each\n",
    "                                hidden layer.\n",
    "            @param n_epochs     Numer of epochs for which to train the\n",
    "                                network. The recommended value 40 should be\n",
    "                                enough.\n",
    "            @param normalize    Whether to normalize the input features. This\n",
    "                                is recommended unles the input vector is for\n",
    "                                example formed by binary features (a\n",
    "                                fingerprint). In that case we do not recommend\n",
    "                                to normalize the features.\n",
    "            @param tau          Tau value used for regularization\n",
    "            @param dropout      Dropout rate for all the dropout layers in the\n",
    "                                network.\n",
    "        \"\"\"\n",
    "        # We normalize the training data to have zero mean and unit standard\n",
    "        # deviation in the training set if necessary\n",
    "\n",
    "#         if normalize:\n",
    "#             self.std_X_train = np.std(X_train, 0)\n",
    "#             self.std_X_train[ self.std_X_train == 0 ] = 1\n",
    "#             self.mean_X_train = np.mean(X_train, 0)\n",
    "#         else:\n",
    "#             self.std_X_train = np.ones(X_train.shape[ 1 ])\n",
    "#             self.mean_X_train = np.zeros(X_train.shape[ 1 ])\n",
    "\n",
    "#         X_train = (X_train - np.full(X_train.shape, self.mean_X_train)) / \\\n",
    "#             np.full(X_train.shape, self.std_X_train)\n",
    "\n",
    "#         self.mean_y_train = np.mean(y_train)\n",
    "#         self.std_y_train = np.std(y_train)\n",
    "\n",
    "#         y_train_normalized = (y_train - self.mean_y_train) / self.std_y_train\n",
    "#         y_train_normalized = np.array(y_train_normalized, ndmin = 2).T\n",
    "        \n",
    "        # We construct the network\n",
    "        N = X_train.shape[0]\n",
    "        batch_size = 128\n",
    "        lengthscale = 1e-2\n",
    "        reg = lengthscale**2 * (1 - dropout) / (2. * N * tau)\n",
    "\n",
    "        inputs = Input(shape=(X_train.shape[1],))\n",
    "        inter = Dropout(dropout)(inputs, training=True)\n",
    "        inter = Dense(n_hidden[0], activation='relu', kernel_regularizer=l2(reg))(inter)\n",
    "        for i in range(1, len(n_hidden) - 1):\n",
    "            inter = Dropout(dropout)(inter, training=True)\n",
    "            inter = Dense(n_hidden[i], activation='relu', kernel_regularizer=l2(reg))(inter)\n",
    "        inter = Dropout(dropout)(inter, training=True)\n",
    "        outputs = Dense(2, kernel_regularizer=l2(reg))(inter)\n",
    "        outputs = Softmax()(outputs)\n",
    "        model = Model(inputs, outputs)\n",
    "\n",
    "        model.compile(loss='binary_crossentropy', optimizer='adam')\n",
    "#         print(model.summary())\n",
    "        # We iterate the learning process\n",
    "        start_time = time.time()\n",
    "        model.fit(X_train, y_train, batch_size=batch_size, epochs=n_epochs, verbose=0)\n",
    "        self.model = model\n",
    "        self.tau = tau\n",
    "        self.running_time = time.time() - start_time\n",
    "\n",
    "        # We are done!\n",
    "\n",
    "    def predict(self, X_test, y_test):\n",
    "\n",
    "        \"\"\"\n",
    "            Function for making predictions with the Bayesian neural network.\n",
    "\n",
    "            @param X_test   The matrix of features for the test data\n",
    "            \n",
    "    \n",
    "            @return m       The predictive mean for the test target variables.\n",
    "            @return v       The predictive variance for the test target\n",
    "                            variables.\n",
    "            @return v_noise The estimated variance for the additive noise.\n",
    "\n",
    "        \"\"\"\n",
    "        X_test = np.array(X_test, ndmin = 2)\n",
    "        y_test = np.array(y_test, ndmin = 2).T\n",
    "\n",
    "        # We normalize the test set\n",
    "\n",
    "#         X_test = (X_test - np.full(X_test.shape, self.mean_X_train)) / \\\n",
    "#             np.full(X_test.shape, self.std_X_train)\n",
    "\n",
    "        # We compute the predictive mean and variance for the target variables\n",
    "        # of the test data\n",
    "\n",
    "        model = self.model\n",
    "        standard_pred_probs = model.predict(X_test, batch_size=500, verbose=1)\n",
    "        standard_pred = tf.math.argmax(standard_pred_probs, axis=1).numpy()\n",
    "        # standard_pred = standard_pred * self.std_y_train + self.mean_y_train\n",
    "        # rmse_standard_pred = np.mean((y_test.squeeze() - standard_pred.squeeze())**2.)**0.5\n",
    "        accuracy_standard_pred = np.mean((y_test.squeeze() == standard_pred.squeeze()))\n",
    "        print(f'Standard Accuracy: {accuracy_standard_pred}')\n",
    "\n",
    "        # Number of stochastic forward passes which will then be averaged\n",
    "        # originally 10_000 in the paper's code. Set to 100 here for speed.\n",
    "        T = 100\n",
    "        \n",
    "        Yt_hat = np.array([model.predict(X_test, batch_size=64, verbose=0) for _ in range(T)])\n",
    "        # Yt_hat = Yt_hat * self.std_y_train + self.mean_y_train\n",
    "        MC_pred = tf.math.argmax(np.mean(Yt_hat, axis=0), axis=1).numpy()\n",
    "        # print(MC_pred.shape)\n",
    "        mc_accuracy = np.mean((y_test.squeeze() == MC_pred.squeeze()))\n",
    "        print(f'MC Accuracy: {mc_accuracy}')\n",
    "\n",
    "        # We compute the test log-likelihood\n",
    "        # ll = (logsumexp(-0.5 * self.tau * (y_test[None] - Yt_hat)**2., 0) - np.log(T) \n",
    "        #     - 0.5*np.log(2*np.pi) + 0.5*np.log(self.tau))\n",
    "        # test_ll = np.mean(ll)\n",
    "        # ll = np.sum(y_test[])\n",
    "        \n",
    "        # double check this!\n",
    "        y_test = y_test.astype(int)\n",
    "        y_test_2d = np.hstack((y_test, 1-y_test))\n",
    "        test_ll = -np.mean(np.log(standard_pred_probs) * y_test_2d)\n",
    "\n",
    "        # We are done!\n",
    "        print(f'Test LL: {test_ll}')\n",
    "        return accuracy_standard_pred, mc_accuracy, test_ll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed3c7981",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2016, Yarin Gal, All rights reserved.\n",
    "# This code is based on the code by Jose Miguel Hernandez-Lobato used for his \n",
    "# paper \"Probabilistic Backpropagation for Scalable Learning of Bayesian Neural Networks\".\n",
    "\n",
    "# This file contains code to train dropout networks on the UCI datasets using the following algorithm:\n",
    "# 1. Create 20 random splits of the training-test dataset.\n",
    "# 2. For each split:\n",
    "# 3.   Create a validation (val) set taking 20% of the training set.\n",
    "# 4.   Get best hyperparameters: dropout_rate and tau by training on (train-val) set and testing on val set.\n",
    "# 5.   Train a network on the entire training set with the best pair of hyperparameters.\n",
    "# 6.   Get the performance (MC RMSE and log-likelihood) on the test set.\n",
    "# 7. Report the averaged performance (Monte Carlo RMSE and log-likelihood) on all 20 splits.\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# We fix the random seed\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "def write_to_file(filename, txt, dropout_rate, tau):\n",
    "    with open(filename, \"a\") as myfile:\n",
    "        myfile.write('Dropout_Rate: ' + repr(dropout_rate) + ' Tau: ' + repr(tau) + ' :: ')\n",
    "        myfile.write(repr(txt) + '\\n')\n",
    "\n",
    "        \n",
    "def run_half_moons_experiment(X, y, hidden_layers, n_epochs, epoch_multiplier, dropout_rates, taus, normalize):\n",
    "    num_training_examples = int(0.8 * X.shape[0])\n",
    "    X_validation = X[num_training_examples:, :]\n",
    "    y_validation = y[num_training_examples:]\n",
    "    X_train = X[0:num_training_examples, :]\n",
    "    y_train = y[0:num_training_examples]\n",
    "    \n",
    "    # Printing the size of the training, validation and test sets\n",
    "    print('Number of training examples: ' + str(X_train.shape[0]))\n",
    "    print('Number of validation examples: ' + str(X_validation.shape[0]))\n",
    "    print('Number of test examples: ' + str(X_test.shape[0]))\n",
    "    print('Number of train_original examples: ' + str(X.shape[0]))\n",
    "    print(f'Dropout rates: {dropout_rates}')\n",
    "    print(f'Taus: {taus}')\n",
    "    \n",
    "    # We perform grid-search to select the best hyperparameters based on the highest log-likelihood value\n",
    "    best_network = None\n",
    "    best_ll = -float('inf')\n",
    "    best_tau = None\n",
    "    best_dropout = None\n",
    "    for dropout_rate in dropout_rates:\n",
    "        for tau in taus:\n",
    "            print ('Grid search step: Tau: ' + str(tau) + ' Dropout rate: ' + str(dropout_rate))\n",
    "            network = net(X_train, y_train, hidden_layers,\n",
    "                              normalize = normalize,\n",
    "                              n_epochs = int(n_epochs * epochs_multiplier),\n",
    "                              tau = tau,\n",
    "                              dropout = dropout_rate\n",
    "                             )\n",
    "            print('DONE TRAINING')\n",
    "\n",
    "            # We obtain the test accuracy and the test ll from the validation sets\n",
    "            accuracy, MC_accuracy, ll = network.predict(X_validation, y_validation)\n",
    "            print('DONE PREDICTING')\n",
    "            if (ll > best_ll):\n",
    "                best_ll = ll\n",
    "                best_network = network\n",
    "                best_tau = tau\n",
    "                best_dropout = dropout_rate\n",
    "                print ('Best log_likelihood changed to: ' + str(best_ll))\n",
    "                print ('Best tau changed to: ' + str(best_tau))\n",
    "                print ('Best dropout rate changed to: ' + str(best_dropout))\n",
    "            \n",
    "            # Storing validation results\n",
    "            write_to_file(_RESULTS_VALIDATION_ACC, accuracy, dropout_rate, tau)\n",
    "            write_to_file(_RESULTS_VALIDATION_MC_ACC, MC_accuracy, dropout_rate, tau)\n",
    "            write_to_file(_RESULTS_VALIDATION_LL, ll, dropout_rate, tau)\n",
    "            \n",
    "\n",
    "    # Storing test results\n",
    "    best_network = net(X_train, y_train, hidden_layers,\n",
    "                    normalize = True, n_epochs = int(n_epochs * epochs_multiplier), tau = best_tau,\n",
    "                    dropout = best_dropout)\n",
    "    \n",
    "    return best_network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22b35607",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory = 'half_moons'\n",
    "\n",
    "# We delete previous results\n",
    "\n",
    "from subprocess import call\n",
    "\n",
    "epochs_multiplier = 1\n",
    "num_hidden_layers = 2\n",
    "n_splits = 20\n",
    "hidden_layers = [50] * num_hidden_layers\n",
    "n_epochs = 50\n",
    "epochs_multiplier = 100\n",
    "dropout_rates = [0.005, 0.01, 0.05, 0.1]\n",
    "taus = [0.25, 0.5, 0.75]\n",
    "\n",
    "_RESULTS_VALIDATION_LL = \"./UCI_Datasets/\" + data_directory + \"/results/validation_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_RMSE = \"./UCI_Datasets/\" + data_directory + \"/results/validation_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_MC_RMSE = \"./UCI_Datasets/\" + data_directory + \"/results/validation_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_ACC = \"./UCI_Datasets/\" + data_directory + \"/results/validation_acc_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_VALIDATION_MC_ACC = \"./UCI_Datasets/\" + data_directory + \"/results/validation_MC_acc_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "\n",
    "_RESULTS_TEST_LL = \"./UCI_Datasets/\" + data_directory + \"/results/test_ll_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_TAU = \"./UCI_Datasets/\" + data_directory + \"/results/test_tau_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_RMSE = \"./UCI_Datasets/\" + data_directory + \"/results/test_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_MC_RMSE = \"./UCI_Datasets/\" + data_directory + \"/results/test_MC_rmse_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_LOG = \"./UCI_Datasets/\" + data_directory + \"/results/log_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_ACC = \"./UCI_Datasets/\" + data_directory + \"/results/test_acc_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "_RESULTS_TEST_MC_ACC = \"./UCI_Datasets/\" + data_directory + \"/results/test_MC_acc_\" + str(epochs_multiplier) + \"_xepochs_\" + str(num_hidden_layers) + \"_hidden_layers.txt\"\n",
    "\n",
    "_DATA_DIRECTORY_PATH = \"./UCI_Datasets/\" + data_directory + \"/data/\"\n",
    "_DROPOUT_RATES_FILE = _DATA_DIRECTORY_PATH + \"dropout_rates.txt\"\n",
    "_TAU_VALUES_FILE = _DATA_DIRECTORY_PATH + \"tau_values.txt\"\n",
    "_DATA_FILE = _DATA_DIRECTORY_PATH + \"data.txt\"\n",
    "_HIDDEN_UNITS_FILE = _DATA_DIRECTORY_PATH + \"n_hidden.txt\"\n",
    "_EPOCHS_FILE = _DATA_DIRECTORY_PATH + \"n_epochs.txt\"\n",
    "_INDEX_FEATURES_FILE = _DATA_DIRECTORY_PATH + \"index_features.txt\"\n",
    "_INDEX_TARGET_FILE = _DATA_DIRECTORY_PATH + \"index_target.txt\"\n",
    "_N_SPLITS_FILE = _DATA_DIRECTORY_PATH + \"n_splits.txt\"\n",
    "\n",
    "print (\"Removing existing result files...\")\n",
    "call([\"rm\", _RESULTS_VALIDATION_LL])\n",
    "call([\"rm\", _RESULTS_VALIDATION_RMSE])\n",
    "call([\"rm\", _RESULTS_VALIDATION_MC_RMSE])\n",
    "call([\"rm\", _RESULTS_VALIDATION_ACC])\n",
    "call([\"rm\", _RESULTS_VALIDATION_MC_ACC])\n",
    "call([\"rm\", _RESULTS_TEST_LL])\n",
    "call([\"rm\", _RESULTS_TEST_TAU])\n",
    "call([\"rm\", _RESULTS_TEST_RMSE])\n",
    "call([\"rm\", _RESULTS_TEST_MC_RMSE])\n",
    "call([\"rm\", _RESULTS_TEST_ACC])\n",
    "call([\"rm\", _RESULTS_TEST_MC_ACC])\n",
    "call([\"rm\", _RESULTS_TEST_LOG])\n",
    "print (\"Result files removed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab60b8ce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_moons\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X, y = make_moons(500, noise=0.1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y)\n",
    "plt.show()\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# encode class values as integers\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(y)\n",
    "encoded_Y = encoder.transform(y)\n",
    "print(encoded_Y)\n",
    "\n",
    "accuracies, MC_accuracies, lls = [], [], []\n",
    "for i in range(n_splits):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, encoded_Y, train_size=0.8, random_state=100)\n",
    "    print(y_train.mean())\n",
    "    print(y_test.mean())\n",
    "    \n",
    "    # tune on train and validation sets, returning best trained neural network\n",
    "    best_network_trained =  run_half_moons_experiment(X_train,\n",
    "                                                      y_train,\n",
    "                                                      hidden_layers,\n",
    "                                                      n_epochs,\n",
    "                                                      epochs_multiplier,\n",
    "                                                      dropout_rates,\n",
    "                                                      taus,\n",
    "                                                      normalize=False\n",
    "                                                     )\n",
    "    \n",
    "    # predict on held out test set\n",
    "    accuracy, MC_accuracy, ll = best_network_trained.predict(X_test, y_test)\n",
    "    \n",
    "    with open(_RESULTS_TEST_ACC, \"a\") as myfile:\n",
    "        myfile.write(repr(accuracy) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_MC_ACC, \"a\") as myfile:\n",
    "        myfile.write(repr(MC_accuracy) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_LL, \"a\") as myfile:\n",
    "        myfile.write(repr(ll) + '\\n')\n",
    "\n",
    "    with open(_RESULTS_TEST_TAU, \"a\") as myfile:\n",
    "        myfile.write(repr(best_network_trained.tau) + '\\n')\n",
    "\n",
    "    print (\"Tests on split \" + str(i) + \" complete.\")\n",
    "    \n",
    "    accuracies.append(accuracy)\n",
    "    MC_accuracies.append(MC_accuracy)\n",
    "    lls.append(ll)\n",
    "\n",
    "with open(_RESULTS_TEST_LOG, \"a\") as myfile:\n",
    "    myfile.write('accuracies %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(accuracies), np.std(accuracies), np.std(accuracies)/math.sqrt(n_splits),\n",
    "        np.percentile(accuracies, 50), np.percentile(accuracies, 25), np.percentile(accuracies, 75)))\n",
    "    myfile.write('MC accuracies %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(MC_accuracies), np.std(MC_accuracies), np.std(MC_accuracies)/math.sqrt(n_splits),\n",
    "        np.percentile(MC_accuracies, 50), np.percentile(MC_accuracies, 25), np.percentile(MC_accuracies, 75)))\n",
    "    myfile.write('lls %f +- %f (stddev) +- %f (std error), median %f 25p %f 75p %f \\n' % (\n",
    "        np.mean(lls), np.std(lls), np.std(lls)/math.sqrt(n_splits), \n",
    "        np.percentile(lls, 50), np.percentile(lls, 25), np.percentile(lls, 75)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994e778",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "682074ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
